{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg_Sp7Xu7n3j"
      },
      "source": [
        "\n",
        "## Importing JIGSAWS dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkLej_ZU7n3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420a199a-88cc-4447-dc74-8221bfa536b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jigsaws\n",
            "/content/jigsaws\n"
          ]
        }
      ],
      "source": [
        "# Creating directory named JIGSAWS\n",
        "!mkdir jigsaws\n",
        "%cd jigsaws\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://cs.jhu.edu/~los/jigsaws/dwnld/DAqBjdiRPB8xXnYpsplWSmqmg96JKe6x/Knot_Tying.zip\n",
        "# !wget https://cs.jhu.edu/~los/jigsaws/dwnld/DAqBjdiRPB8xXnYpsplWSmqmg96JKe6x/Needle_Passing.zip\n",
        "# !wget https://cs.jhu.edu/~los/jigsaws/dwnld/DAqBjdiRPB8xXnYpsplWSmqmg96JKe6x/Suturing.zip\n",
        "# !wget https://cs.jhu.edu/~los/jigsaws/dwnld/DAqBjdiRPB8xXnYpsplWSmqmg96JKe6x/Experimental_setup.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPrz4nvy8eR1",
        "outputId": "90a5bed5-9d63-4a37-b6ac-74ba981d56bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-29 07:53:10--  https://cs.jhu.edu/~los/jigsaws/dwnld/DAqBjdiRPB8xXnYpsplWSmqmg96JKe6x/Knot_Tying.zip\n",
            "Resolving cs.jhu.edu (cs.jhu.edu)... 128.220.13.64\n",
            "Connecting to cs.jhu.edu (cs.jhu.edu)|128.220.13.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.cs.jhu.edu/~los/jigsaws/dwnld/DAqBjdiRPB8xXnYpsplWSmqmg96JKe6x/Knot_Tying.zip [following]\n",
            "--2023-01-29 07:53:11--  https://www.cs.jhu.edu/~los/jigsaws/dwnld/DAqBjdiRPB8xXnYpsplWSmqmg96JKe6x/Knot_Tying.zip\n",
            "Resolving www.cs.jhu.edu (www.cs.jhu.edu)... 128.220.13.64\n",
            "Connecting to www.cs.jhu.edu (www.cs.jhu.edu)|128.220.13.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 73622618 (70M) [application/zip]\n",
            "Saving to: ‘Knot_Tying.zip’\n",
            "\n",
            "Knot_Tying.zip      100%[===================>]  70.21M  13.5MB/s    in 6.6s    \n",
            "\n",
            "2023-01-29 07:53:20 (10.6 MB/s) - ‘Knot_Tying.zip’ saved [73622618/73622618]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "!unzip -o Knot_Tying.zip -d jigsaws\n",
        "# !unzip -o Needle_Passing.zip -d jigsaws\n",
        "# !unzip -o Suturing.zip -d jigsaws\n",
        "# !unzip -o Experimental_setup.zip -d jigsaws\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "wF0XqpHx9PoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huWg0h3C7n3k"
      },
      "source": [
        "## Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOepjT8n7n3k"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wfQ3tFZ7n3l"
      },
      "source": [
        "## Define hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c52T8xvr7n3m"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "MAX_SEQ_LENGTH = 20\n",
        "NUM_FEATURES = 2048\n",
        "EXP_TYPE = 'Knot_Tying'\n",
        "ROOT_DIR = '/content/jigsaws/jigsaws'\n",
        "COLNAMES = ['video_name','tag']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrldOUtU7n3m"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getSkills():\n",
        "  skills = {}\n",
        "  with open(os.path.join(ROOT_DIR,EXP_TYPE,f'meta_file_{EXP_TYPE}.txt'),'r') as f:\n",
        "    for line in f.readlines()[:-1]:\n",
        "      l = (line.strip()).split(\"\\t\")\n",
        "      skills[l[0]] = l[2]\n",
        "  return skills"
      ],
      "metadata": {
        "id": "EoU_ujWL-hTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getMetadata():\n",
        "  data = []\n",
        "  skills = getSkills()\n",
        "  videos = os.listdir(os.path.join(ROOT_DIR,EXP_TYPE,'video'));\n",
        "  print(f\"Total videos in {EXP_TYPE}: \",len(videos))\n",
        "  for vid in videos:\n",
        "    l = vid.split('_')\n",
        "    data.append(\n",
        "        [vid,skills[l[0]+\"_\"+l[1]]]\n",
        "    )\n",
        "  return pd.DataFrame(data = data,columns = COLNAMES)"
      ],
      "metadata": {
        "id": "ZRoFM8ndDMk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = getMetadata()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "-RWZlW7QIE9u",
        "outputId": "01291f1c-4d9b-47c7-c4f9-923f35f5dada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ac5a34eca281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-6f08b9e7218b>\u001b[0m in \u001b[0;36mgetMetadata\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mskills\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSkills\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mvideos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEXP_TYPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'video'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total videos in {EXP_TYPE}: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-07c20205c461>\u001b[0m in \u001b[0;36mgetSkills\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetSkills\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mskills\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEXP_TYPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf'meta_file_{EXP_TYPE}.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/jigsaws/jigsaws/Knot_tying/meta_file_Knot_tying.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "2G8SgRBJJ5eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stratified sampling of videos\n",
        "\n"
      ],
      "metadata": {
        "id": "ZAaHoyB1K4jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_indices,test_indices in sss.split(data.iloc[:,0],data.iloc[:,1]):\n",
        "  train_df = data.iloc[train_indices]\n",
        "  test_df = data.iloc[test_indices]"
      ],
      "metadata": {
        "id": "FzY48zc-HX50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['tag'].value_counts()"
      ],
      "metadata": {
        "id": "LBaFXYhENBft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['tag'].value_counts()"
      ],
      "metadata": {
        "id": "KjLNFjFBPWy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ju9Oj66H7n3m"
      },
      "outputs": [],
      "source": [
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "train_df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5sKFIwL7n3n"
      },
      "source": [
        "**Workflow**\n",
        "\n",
        "1. Capture the frames of a video.\n",
        "2. Extract frames from the videos until a maximum frame count is reached.\n",
        "3. In the case, where a video's frame count is lesser than the maximum frame count we\n",
        "will pad the video with zeros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2aUbRre7n3n"
      },
      "source": [
        "We can use a pre-trained network **Xception** to extract meaningfulfeatures from the extracted frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYysmTQr7n3n"
      },
      "outputs": [],
      "source": [
        "def build_feature_extractor():\n",
        "  feature_extractor = keras.applications.Xception(\n",
        "      weights=\"imagenet\",\n",
        "      include_top=False,\n",
        "      pooling=\"avg\",\n",
        "      input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "  )\n",
        "  preprocess_input = keras.applications.xception.preprocess_input\n",
        "\n",
        "  inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "  preprocessed = preprocess_input(inputs)\n",
        "\n",
        "  outputs = feature_extractor(preprocessed)\n",
        "  return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGBj-2Ji7n3o"
      },
      "source": [
        "The labels of the videos are strings. Neural networks do not understand string values,\n",
        "so they must be converted to some numerical form before they are fed to the model. Here\n",
        "we will use the [`StringLookup`](https://keras.io/api/layers/preprocessing_layers/categorical/string_lookup)\n",
        "layer encode the class labels as integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLePM7Vu7n3o"
      },
      "outputs": [],
      "source": [
        "label_processor = keras.layers.StringLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"])\n",
        ")\n",
        "print(label_processor.get_vocabulary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgsZDRP67n3o"
      },
      "source": [
        "Finally, we can put all the pieces together to create our data processing utility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khwQ_HAI7n3n"
      },
      "outputs": [],
      "source": [
        "def crop_center_square(frame):\n",
        "    y, x = frame.shape[0:2]\n",
        "    min_dim = min(y, x)\n",
        "    start_x = (x // 2) - (min_dim // 2)\n",
        "    start_y = (y // 2) - (min_dim // 2)\n",
        "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
        "\n",
        "\n",
        "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center_square(frame)\n",
        "            frame = cv2.resize(frame, resize)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vD6OEiZH7n3o"
      },
      "outputs": [],
      "source": [
        "def generate_data(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"tag\"].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
        "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
        "    # masked with padding or not.\n",
        "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholders to store the masks and features of the current video.\n",
        "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                    batch[None, j, :]\n",
        "                )\n",
        "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
        "\n",
        "    return (frame_features, frame_masks), labels\n",
        "\n",
        "\n",
        "train_data, train_labels = generate_data(train_df, os.path.join(ROOT_DIR,EXP_TYPE,'video'))\n",
        "test_data, test_labels = generate_data(test_df, os.path.join(ROOT_DIR,EXP_TYPE,'video'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
        "print(f\"Frame masks in train set: {train_data[1].shape}\")"
      ],
      "metadata": {
        "id": "g8iuf7CX8p4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRvs_Vd77n3p"
      },
      "source": [
        "## The video classifier model\n",
        "\n",
        "Now, we can feed this data to a sequence model consisting of recurrent layers like `GRU`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO-TlUIv7n3p"
      },
      "outputs": [],
      "source": [
        "# Code for generating model\n",
        "def Classifier():\n",
        "    class_vocab = label_processor.get_vocabulary()\n",
        "\n",
        "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
        "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "\n",
        "    x = keras.layers.GRU(32, return_sequences=True)(\n",
        "        frame_features_input, mask=mask_input\n",
        "    )\n",
        "    x = keras.layers.GRU(16)(x)\n",
        "    x = keras.layers.Dropout(0.2)(x)\n",
        "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
        "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
        "\n",
        "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
        "\n",
        "    rnn_model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return rnn_model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training our video classifier\n",
        "filepath = \"/tmp/video_classifier\"\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "    filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        ")\n",
        "\n",
        "model = Classifier()\n",
        "history = model.fit(\n",
        "    [train_data[0], train_data[1]],\n",
        "    train_labels,\n",
        "    validation_split=0.3,\n",
        "    epochs=100,\n",
        "    callbacks=[checkpoint],\n",
        ")\n",
        "\n",
        "model.load_weights(filepath)\n",
        "\n",
        "_, accuracy = model.evaluate([test_data[0], test_data[1]], test_labels)\n",
        "\n",
        "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
      ],
      "metadata": {
        "id": "CHO1zA9u3JUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict([test_data[0], test_data[1]])"
      ],
      "metadata": {
        "id": "v8EY1Yje3sVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model,show_shapes = True)"
      ],
      "metadata": {
        "id": "nBRB0kFqBqkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkIhR4eE7n3p"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "cnf = confusion_matrix(np.squeeze(test_labels),np.argmax(y_pred,axis = 1))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cnf, annot=True, fmt='g', ax=ax, cmap = 'cividis');  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix'); \n",
        "ticks = label_processor.get_vocabulary()\n",
        "ax.xaxis.set_ticklabels(ticks); ax.yaxis.set_ticklabels(ticks)"
      ],
      "metadata": {
        "id": "nPQpUyFR7WoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A-TQ_IHk45NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "\n",
        "plt.figure(figsize = (12,8))\n",
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "classes = label_processor.get_vocabulary()\n",
        "n_classes = len(label_processor.get_vocabulary())\n",
        "y_test = np.squeeze(test_labels)\n",
        "y_test = label_binarize(y_test,classes = np.unique(y_test))\n",
        "\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    plt.plot(fpr[i],tpr[i], label = f\"{classes[i]} vs Rest  AUC = {roc_auc[i]:.2f}\")\n",
        "\n",
        "plt.plot([0,1],[0,1],'b--')\n",
        "plt.xlim([0,1])\n",
        "plt.ylim([0,1.05])\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.xlabel('FPR')\n",
        "plt.ylabel('TPR')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w74RK41kM-69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving model \n",
        "model.save(\"CNN_RNN.h5\")"
      ],
      "metadata": {
        "id": "jO21rudlFLb0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}